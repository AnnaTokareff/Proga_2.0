import os
import re
import gc
import glob
import json 
import nltk
import string
import pymorphy2
import pandas as pd  
from nltk.corpus import stopwords 
from nltk.tokenize import sent_tokenize, word_tokenize
from sklearn.feature_extraction.text import CountVectorizer



s_words = set(stopwords.words('russian'))
s_words.update(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', 'к', 'на','по','у'])
morph = pymorphy2.MorphAnalyzer()

class summarizer:
    def __init__(self,text=""):
        self.train = None
        self.test = None
        self.valid= None
        self.sent = []
        self.text = text
        


    def open_files(self):
    	with open("gazeta_train.jsonl","rb") as f2:
    		self.train=pd.read_json(f2,lines=True)
    		gc.collect()
    	#with open("gazeta_test.jsonl","rb") as f1:
    	#	self.test=pd.read_json(f1, lines=True)
    		#with open("gazeta_train.jsonl","rb") as f2:
    		#	self.train=pd.read_json(f2,lines=True)
	    		#open("gazeta_val.jsonl","rb") as f3:
    			#self.valid=pd.read_json(f3,lines=True)
    	return (self.train)

    def __repr__(self):
    	return (self.test.info(),self.train.info(),self.valid.info())


    #def preprocess_text(self):
    #	tokens = [w for sent in nltk.sent_tokenize(text) for w in nltk.word_tokenize(sent)]
    #	tokens = [w for w in tokens if w not in s_words]
    #	tokens = [w for w in tokens if w.isalpha() == True]
    #	for word in tokens:
    #		p = morph.parse(word)[0]
    #		self.tokens.append(p.normal_form)
    #	return tokens

    
    def preprocess_text(self,text):

        ''' Предобработка текста, возвращает список предложений'''
        text = text.lower()
        text = re.sub(r"\d+", "", text)
        text = re.sub(r'[^\w\s\\.]','', text) 
        sent = [sent.strip() for sent in nltk.sent_tokenize(text)]
        self.sent = [sentence.replace("\n","") for sentence in sent]
        return (self.sent)


    def fit_clustering(self):
        '''Обучение модели '''
        preprocess_text = self.preprocess_text
        vec=CountVectorizer(tokenizer=lambda x: preprocess_text(x))
        matrix = vec.fit_transform(self.train["text"])
        vocab = vec.get_feature_names()
    	#print(matrix)






reader = summarizer()
reader.open_files()
#reader.__repr__()
#reader.preprocess_text()
reader.fit_clustering()


